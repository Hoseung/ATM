{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98384132-484e-419a-9833-2381fee5102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "import atm\n",
    "import atm.simclr as simclr\n",
    "import atm.simclr.resnet as models\n",
    "\n",
    "import argparse \n",
    "\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04fca200-fa2a-489d-8731-2bbcf98854ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "do_parallel = False\n",
    "\n",
    "args = argparse.Namespace()\n",
    "\n",
    "args.data='./datasets' \n",
    "args.dataset_name=['cifar10', 'stl10', 'nair'][0]\n",
    "args.arch='resnet50'\n",
    "args.workers=1\n",
    "args.epochs=200 \n",
    "args.img_size =128\n",
    "args.n_channels=1\n",
    "\n",
    "if do_parallel:\n",
    "    args.batch_size = 128\n",
    "else:\n",
    "    args.batch_size = 256\n",
    "\n",
    "args.lr=0.02\n",
    "args.weight_decay=0.0005\n",
    "args.disable_cuda=False\n",
    "args.fp16_precision=True\n",
    "args.out_dim=10\n",
    "args.log_every_n_steps=100\n",
    "args.temperature=0.07\n",
    "args.n_views = 2\n",
    "args.device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"Using device:\", args.device)\n",
    "\n",
    "assert args.n_views == 2, \"Only two view training is supported. Please use --n-views 2.\"\n",
    "# check if gpu training is available\n",
    "if not args.disable_cuda and torch.cuda.is_available():\n",
    "    args.device = torch.device('cuda')\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    args.device = torch.device('cpu')\n",
    "    #args.gpu_index = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1590c856-37db-4b9f-9804-f48474296b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from typing import Any, Callable, Optional, Tuple\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from functools import partial\n",
    "from astrobf.tmo import Mantiuk_Seidel\n",
    "\n",
    "class ContrastiveLearningViewGenerator(object):\n",
    "    \"\"\"Take two random crops of one image as the query and key.\"\"\"\n",
    "\n",
    "    def __init__(self, base_transform, n_views=2):\n",
    "        self.base_transform = base_transform\n",
    "        self.n_views = n_views\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return [self.base_transform(x) for i in range(self.n_views)]\n",
    "    \n",
    "class TonemapImageDataset(VisionDataset):\n",
    "    def __init__(self, \n",
    "                 data_array, \n",
    "                 tmo,\n",
    "                 labels: Optional = None, \n",
    "                 train: bool=True,\n",
    "                 transform: Optional[Callable] = None,\n",
    "                 target_transform: Optional[Callable] = None,):\n",
    "        self._array = data_array\n",
    "        self._good_gids = np.array([gal['img_name'] for gal in data_array])\n",
    "        self.img_labels = labels\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.tmo = tmo\n",
    "        self._bad_tmo=False\n",
    "\n",
    "    def _apply_tm(self, image):\n",
    "        try:\n",
    "            return self.tmo(image)\n",
    "        except ZeroDivisionError:\n",
    "            print(\"division by zero. Probably bad choice of TM parameters\")\n",
    "            self._bad_tmo=True\n",
    "            return image\n",
    "\n",
    "    def _to_8bit(self, image):\n",
    "        \"\"\"\n",
    "        Normalize per image (or use global min max??)\n",
    "        \"\"\"\n",
    "\n",
    "        image = (image - image.min())/image.ptp()\n",
    "        image *= 255\n",
    "        return image.astype('uint8')        \n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self._array)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        For super\n",
    "        \"\"\"\n",
    "        image, _segmap, weight = self._array[idx]['data']\n",
    "        image[~_segmap.astype(bool)] = 0#np.nan # Is it OK to have nan?\n",
    "        image[image < 0] = 0\n",
    "\n",
    "        image = self._to_8bit(self._apply_tm(image))\n",
    "        image = Image.fromarray(image)\n",
    "        target = self.img_labels[idx]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "            \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53c59152-bed5-42c4-9850-a2a0b1c0a8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data   \n",
    "import pickle\n",
    "from astrobf.utils.misc import load_Nair\n",
    "\n",
    "ddir = \"../../tonemap/bf_data/Nair_and_Abraham_2010/\"\n",
    "fn = ddir + \"all_gals.pickle\"\n",
    "all_gals = pickle.load(open(fn, \"rb\"))\n",
    "#all_gals = all_gals[1:] # Why the first galaxy image is NaN?\n",
    "good_gids = np.array([gal['img_name'] for gal in all_gals])\n",
    "\n",
    "# Catalog\n",
    "cat_data = load_Nair(ddir + \"catalog/table2.dat\")\n",
    "cat = cat_data[cat_data['ID'].isin(good_gids)] # pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f047ca3-8514-47fd-8206-7e02271591ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from atm.simclr.utils import save_config_file, accuracy, save_checkpoint\n",
    "class ResNetSimCLR(nn.Module):\n",
    "\n",
    "    def __init__(self, base_model, out_dim, n_channels=3):\n",
    "        super(ResNetSimCLR, self).__init__()\n",
    "        self.resnet_dict = {\"resnet18\": models.resnet18(pretrained=False, num_classes=out_dim,\n",
    "                                                        num_channels=n_channels),\n",
    "                            \"resnet50\": models.resnet50(pretrained=False, num_classes=out_dim,\n",
    "                                                        num_channels=n_channels)}\n",
    "\n",
    "        self.backbone = self._get_basemodel(base_model)\n",
    "        dim_mlp = self.backbone.fc.in_features\n",
    "\n",
    "        # add mlp projection head\n",
    "        self.backbone.fc = nn.Sequential(nn.Linear(dim_mlp, dim_mlp), nn.ReLU(), self.backbone.fc)\n",
    "\n",
    "    def _get_basemodel(self, model_name):\n",
    "        try:\n",
    "            model = self.resnet_dict[model_name]\n",
    "        except KeyError:\n",
    "            raise InvalidBackboneError(\n",
    "                \"Invalid backbone architecture. Check the config file and pass one of: resnet18 or resnet50\")\n",
    "        else:\n",
    "            return model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "\n",
    "class SimCLR(object):\n",
    "    import time\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.args = kwargs['args']\n",
    "        self.model = kwargs['model'].to(self.args.device)\n",
    "        self.optimizer = kwargs['optimizer']\n",
    "        self.scheduler = kwargs['scheduler']\n",
    "        \n",
    "        timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        log_dir = timestr + f\"_{self.args.dataset_name}_{self.args.arch}_{self.args.n_channels}_{self.args.batch_size}\"\n",
    "        self.writer = SummaryWriter(log_dir=log_dir)\n",
    "        logging.basicConfig(filename=os.path.join(self.writer.log_dir, 'training.log'), level=logging.DEBUG)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss().to(self.args.device)\n",
    "\n",
    "    def info_nce_loss(self, features):\n",
    "\n",
    "        labels = torch.cat([torch.arange(self.args.batch_size) for i in range(self.args.n_views)], dim=0)\n",
    "        labels = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()\n",
    "        labels = labels.to(self.args.device)\n",
    "\n",
    "        features = F.normalize(features, dim=1)\n",
    "\n",
    "        similarity_matrix = torch.matmul(features, features.T)\n",
    "\n",
    "        # discard the main diagonal from both: labels and similarities matrix\n",
    "        mask = torch.eye(labels.shape[0], dtype=torch.bool).to(self.args.device)\n",
    "        labels = labels[~mask].view(labels.shape[0], -1)\n",
    "        similarity_matrix = similarity_matrix[~mask].view(similarity_matrix.shape[0], -1)\n",
    "        # assert similarity_matrix.shape == labels.shape\n",
    "\n",
    "        # select and combine multiple positives\n",
    "        positives = similarity_matrix[labels.bool()].view(labels.shape[0], -1)\n",
    "\n",
    "        # select only the negatives the negatives\n",
    "        negatives = similarity_matrix[~labels.bool()].view(similarity_matrix.shape[0], -1)\n",
    "\n",
    "        logits = torch.cat([positives, negatives], dim=1)\n",
    "        labels = torch.zeros(logits.shape[0], dtype=torch.long).to(self.args.device)\n",
    "\n",
    "        logits = logits / self.args.temperature\n",
    "        return logits, labels\n",
    "\n",
    "    def train(self, train_loader, checkpoint_freq=100):\n",
    "\n",
    "        scaler = GradScaler(enabled=self.args.fp16_precision)\n",
    "\n",
    "        # save config file\n",
    "        save_config_file(self.writer.log_dir, self.args)\n",
    "\n",
    "        n_iter = 0\n",
    "        logging.info(f\"Start SimCLR training for {self.args.epochs} epochs.\")\n",
    "        logging.info(f\"Training with gpu: {self.args.disable_cuda}.\")\n",
    "\n",
    "        for epoch_counter in range(self.args.epochs):\n",
    "            for images, _ in tqdm(train_loader):\n",
    "                images = torch.cat(images, dim=0)\n",
    "\n",
    "                images = images.to(self.args.device)\n",
    "\n",
    "                with autocast(enabled=self.args.fp16_precision):\n",
    "                    features = self.model(images)\n",
    "                    logits, labels = self.info_nce_loss(features)\n",
    "                    loss = self.criterion(logits, labels)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                scaler.step(self.optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                if n_iter % self.args.log_every_n_steps == 0:\n",
    "                    top1, top5 = accuracy(logits, labels, topk=(1, 5))\n",
    "                    self.writer.add_scalar('loss', loss, global_step=n_iter)\n",
    "                    self.writer.add_scalar('acc/top1', top1[0], global_step=n_iter)\n",
    "                    self.writer.add_scalar('acc/top5', top5[0], global_step=n_iter)\n",
    "                    self.writer.add_scalar('learning_rate', self.scheduler.get_lr()[0], global_step=n_iter)\n",
    "\n",
    "                n_iter += 1\n",
    "\n",
    "            # warmup for the first 10 epochs\n",
    "            if epoch_counter >= 10:\n",
    "                self.scheduler.step()\n",
    "            logging.debug(f\"Epoch: {epoch_counter}\\tLoss: {loss}\\tTop1 accuracy: {top1[0]}\")\n",
    "            \n",
    "            if epoch_counter % checkpoint_freq == checkpoint_freq -1 or epoch_counter == self.args.epochs:\n",
    "                # save model checkpoints\n",
    "                checkpoint_name = 'checkpoint_{:04d}.pth.tar'.format(self.args.epochs)\n",
    "                save_checkpoint({\n",
    "                    'epoch': self.args.epochs,\n",
    "                    'arch': self.args.arch,\n",
    "                    'dataset':self.args.dataset_name,\n",
    "                    'state_dict': self.model.state_dict(),\n",
    "                    'optimizer': self.optimizer.state_dict(),\n",
    "                    'batchsize': self.args.batch_size,\n",
    "                }, is_best=False, filename=os.path.join(self.writer.log_dir, checkpoint_name))\n",
    "                logging.info(f\"Model checkpoint and metadata has been saved at {self.writer.log_dir}.\")\n",
    "\n",
    "        logging.info(\"Training has finished.\")\n",
    "            \n",
    "\n",
    "def get_simclr_pipeline_transform(size, s=1, n_channels=3):\n",
    "    \"\"\"Return a set of data augmentation transformations as described in the SimCLR paper.\"\"\"\n",
    "    if n_channels == 3:\n",
    "        color_jitter = transforms.ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s)\n",
    "        _transforms = [transforms.RandomResizedCrop(size=size),\n",
    "                      transforms.RandomHorizontalFlip(),\n",
    "                      transforms.RandomApply([color_jitter], p=0.8),\n",
    "                      transforms.RandomGrayscale(p=0.2),\n",
    "                      GaussianBlur(kernel_size=int(0.1 * size)),\n",
    "                      transforms.ToTensor()]\n",
    "    elif n_channels == 1:\n",
    "        _transforms = [transforms.RandomResizedCrop(size=size),\n",
    "                      transforms.RandomHorizontalFlip(),\n",
    "                      #GaussianBlur(kernel_size=int(0.1 * size)),\n",
    "                      transforms.ToTensor(),\n",
    "                      transforms.Lambda(lambda x: x.mean(dim=0, keepdim=True))]\n",
    "    \n",
    "    return transforms.Compose(_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74b9b814-bb4e-467a-896c-8b432318db01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/37 [00:00<?, ?it/s]/home/hoseung/anaconda3/envs/tm39/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/hoseung/anaconda3/envs/tm39/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:508: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "100%|██████████| 37/37 [00:25<00:00,  1.46it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.80it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.78it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.77it/s]\n",
      "100%|██████████| 37/37 [00:21<00:00,  1.72it/s]\n",
      "100%|██████████| 37/37 [00:21<00:00,  1.75it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.77it/s]\n",
      "100%|██████████| 37/37 [00:21<00:00,  1.76it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.77it/s]\n",
      "100%|██████████| 37/37 [00:21<00:00,  1.75it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.77it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.77it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.79it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.77it/s]\n",
      "100%|██████████| 37/37 [00:21<00:00,  1.76it/s]\n",
      "100%|██████████| 37/37 [00:21<00:00,  1.75it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.78it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.80it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.80it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.77it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.79it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.79it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.80it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.80it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.80it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.79it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.78it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.78it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.77it/s]\n",
      "100%|██████████| 37/37 [00:21<00:00,  1.70it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.80it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.79it/s]\n",
      "100%|██████████| 37/37 [00:21<00:00,  1.75it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.76it/s]\n",
      "100%|██████████| 37/37 [00:21<00:00,  1.75it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.80it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.78it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.79it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.80it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.78it/s]\n",
      "100%|██████████| 37/37 [00:21<00:00,  1.74it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.80it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.80it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.79it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.80it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.80it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.80it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.79it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.80it/s]\n",
      "100%|██████████| 37/37 [00:21<00:00,  1.75it/s]\n",
      "100%|██████████| 37/37 [00:21<00:00,  1.76it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.80it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.80it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.80it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.79it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.79it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.79it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.79it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.79it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.81it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.80it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.80it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.80it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.80it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.80it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.80it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.79it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.79it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.80it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.80it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.79it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.80it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.79it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.79it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.83it/s]\n",
      "100%|██████████| 37/37 [00:20<00:00,  1.82it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Namespace' object has no attribute 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b0c4c6aa5a7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#with torch.cuda.device(args.gpu_index):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0msimclr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimCLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0msimclr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#main()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-2c449c072c25>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;34m'arch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0;34m'dataset'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Namespace' object has no attribute 'dataset'"
     ]
    }
   ],
   "source": [
    "tmo_params = {'b': 6.0,  'c': 3.96, 'dl': 9.22, 'dh': 2.45}\n",
    "\n",
    "train_dataset = TonemapImageDataset(all_gals, partial(Mantiuk_Seidel, **tmo_params),\n",
    "                                    labels=cat['TT'].to_numpy(),\n",
    "                                    train=True, \n",
    "                                    transform=ContrastiveLearningViewGenerator(\n",
    "                                        get_simclr_pipeline_transform(128, n_channels=args.n_channels)\n",
    "                                    ))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "    num_workers=args.workers, pin_memory=True, drop_last=True)\n",
    "\n",
    "model = ResNetSimCLR(base_model=args.arch, out_dim=args.out_dim, n_channels=args.n_channels)\n",
    "if do_parallel:\n",
    "    model = nn.DataParallel(model)#, output_device=1) # split works into different devices. 1 deals with the output, 0 does the rest.\n",
    "    # The commented part causes an error:\n",
    "    # Expected all tensors to be on the same device, but found at least two devices,\n",
    "    # cuda:1 and cuda:0! (when checking arugment for argument target in method wrapper_nll_loss_forward)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader), eta_min=0,\n",
    "                                                       last_epoch=-1)\n",
    "\n",
    "\n",
    "np.seterr(divide='ignore')\n",
    "#  It’s a no-op if the 'gpu_index' argument is a negative integer or None.\n",
    "#with torch.cuda.device(args.gpu_index):\n",
    "simclr = SimCLR(model=model, optimizer=optimizer, scheduler=scheduler, args=args)\n",
    "simclr.train(train_loader)\n",
    "        \n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6b48e6-7c1f-43fd-81d1-0a23a581820e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87dc148-c4cf-4e7e-8771-f0d592c9b096",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
